{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import xgboost as xgb\n",
        "import pickle"
      ],
      "metadata": {
        "id": "OxWKBw98sqhr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, data_path, target_column, numeric_columns, categorical_columns, model_type='RandomForest'):\n",
        "        self.data_path = data_path\n",
        "        self.target_column = target_column\n",
        "        self.numeric_columns = numeric_columns\n",
        "        self.categorical_columns = categorical_columns\n",
        "        self.model_type = model_type\n",
        "        self.model = None\n",
        "        self.pipeline = None\n",
        "\n",
        "    def load_data(self):\n",
        "        self.data = pd.read_csv(self.data_path)\n",
        "        self.X = self.data.drop(self.target_column, axis=1)\n",
        "        self.y = self.data[self.target_column]\n",
        "\n",
        "    def setup_pipeline(self):\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),  # Menggunakan median untuk imputasi\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),  # Menggunakan modus untuk imputasi\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, self.numeric_columns),\n",
        "                ('cat', categorical_transformer, self.categorical_columns)\n",
        "            ])\n",
        "\n",
        "        if self.model_type == 'RandomForest':\n",
        "            self.model = RandomForestClassifier(random_state=42)\n",
        "        elif self.model_type == 'XGBoost':\n",
        "            self.model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported model type\")\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', self.model)\n",
        "        ])\n",
        "\n",
        "    def train_model(self):\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
        "        self.pipeline.fit(self.X_train, self.y_train)\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        predictions = self.pipeline.predict(self.X_test)\n",
        "        print(classification_report(self.y_test, predictions))\n",
        "        print(\"Accuracy: \", accuracy_score(self.y_test, predictions))\n",
        "\n",
        "    def save_model(self, file_path):\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(self.pipeline, f)"
      ],
      "metadata": {
        "id": "MPpTqtYOsrig"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    numeric_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
        "    categorical_cols = ['Geography', 'Gender']\n",
        "    trainer = ModelTrainer(data_path='/content/data_D.csv', target_column='churn', numeric_columns=numeric_cols, categorical_columns=categorical_cols, model_type='RandomForest')\n",
        "    trainer.load_data()\n",
        "    trainer.setup_pipeline()\n",
        "    trainer.train_model()\n",
        "    trainer.evaluate_model()\n",
        "    trainer.save_model('/content/best_data (2).pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNZva0GssrmX",
        "outputId": "8b788398-eea6-4e02-9ee9-9a0c9d27f953"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.91      6513\n",
            "           1       0.72      0.50      0.59      1739\n",
            "\n",
            "    accuracy                           0.85      8252\n",
            "   macro avg       0.80      0.73      0.75      8252\n",
            "weighted avg       0.84      0.85      0.84      8252\n",
            "\n",
            "Accuracy:  0.8537324285021813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c7CgrcEW9y25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}